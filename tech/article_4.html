<!DOCTYPE html>
<html lang = "ja">
  <head>
    <meta charset = "UTF-8">
    <meta name="description">
    <title>TECH_NOTES (Issaku Kawashima)</title>
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
    <link href = "prettify.css" rel = "stylesheet" type = "text/css" media = "all">
    <link href = "tech.css" rel = "stylesheet" type = "text/css" media = "all">
  </head>
  <body>
    <header>
      <h1>Pipelineに渡すEstimatorを自作する</h1>
      <p>
        執筆日：2018年5月21日<br>
        環境：Windows10, Python 2.7.14, scikit-learn 0.19.1<br>
      </p>
    </header>
    <section>
      <h2>1. Pipelineについて</h2>
      <p>
        <a href="http://scikit-learn.org/stable/">
        Scikit-learn</a>は、Pythonで機械学習を行うための非常に強力なライブラリです。<br>
        中でも便利な使い方のうちひとつが、
        <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">
        Pipeline</a>と
        <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">
        GridSearchCV</a>のコンボです。
      </p><p>
        GridSearchCVは、交差検証 (Cross-Validation) によってハイパーパラメータ群から適切なものを選ぶ機能です。<br>
        推定器 (Estimator)、パラメータ群、そしてトレーニングデータを与えることで、全パラメータの組み合わせを試して最良のものを選んでくれます。<br>
        仮に推定器として linear な Support Vector Machine を用いる場合、epsilon と C のパラメータを決めてやる必要があります。<br>
				そこで例えば、0.1, 0.2, 0.3,..., 1.9, 2.0 という20個のパラメータ候補を epsilon と C それぞれに用意してやります。<br>
				これらから epsilon と C の パラメータをひとつずつ選ぶ場合、選び方は20x20=400通りになります。<br>
				その400パターン全てのパラメータ設定で交差検証を行い、どの組み合わせが最も良い精度をもたらすか求めるのがGrid Search です。<br>
				そして、これをフルオートでやってくれる便利モジュールがGridSearchCVなのです。
      </p><p>
				SVM へ突っ込む前に PCA にかけて特徴をまとめる、というのはよく使われる手法です。<br>
				その場合、PCA のコンポーネント数もパラメータのひとつとしてサーチしてやりたいところです。<br>
				例えばコンポーネント数を10, 11,..., 19個のいずれかにする場合、epsilon と C と併せ、20x20x10=4,000 通りのパターンを試すわけです。<br>
				このように、複数の処理ステップ (SVM と PCA) をまたいでパラメータ探索をすることを可能にするのがPipelineです。<br>
        「SVM の後に PCAにかける」という一連の流れを、ひとつの推定器として扱ってくれるのです。
      </p><p>
        しかし、Pipeline に突っ込める処理ステップは、fit と transform の メソッドを持つクラス（？）に限られます<br>
        （最後のステップは fit だけでも可）。<br>
        fit は、学習をさせるためのメソッド、transform は、学習をもとに、データから推定を行うメソッドです。<br>
        大概の手法には対応していますが、例えば「観測値との相関係数が一定以上の変数に絞る」なんてことはできないようです<br>
        （そんなもんやる必要ない、ってことかもしれませんが…）<br>
        じゃあそういうクラスを自分で作っちゃおう！というのがこの記事です。
    </section>
    <section>
      <h2>2. モジュールの中身を見てみる</h2>
      <p>
        Pipeline に突っ込める関数をどれか見てみましょう。<br>
        とりあえず、PCAを見てみます。<br>
        [C:\Python27\Lib\site-packages\sklearn\decomposition] にある [pca.py] を開きます。<br>
        Pipeline に突っ込むうえで最低限必要そうな箇所を抽出すると、こんな感じです。<br>
        <pre><code class="prettyprint lang-py">
class RandomizedPCA(BaseEstimator, TransformerMixin):
    def __init__(self, n_components=None, copy=True, iterated_power=2,
                 whiten=False, random_state=None):
        self.n_components = n_components
        self.copy = copy
        self.iterated_power = iterated_power
        self.whiten = whiten
        self.random_state = random_state

    def fit(self, X, y=None):
        self._fit(check_array(X))
        return self

    def _fit(self, X):

        （省略）

        return X

    def transform(self, X):
        check_is_fitted(self, 'mean_')

        X = check_array(X)
        if self.mean_ is not None:
            X = X - self.mean_

        X = np.dot(X, self.components_.T)
        return X

    def fit_transform(self, X, y=None):
        X = check_array(X)
        X = self._fit(X)
        return np.dot(X, self.components_.T)
        </code></pre>
        そんなにややこしくないですね。<br>
        行けそうな気がしてきます。<br>
        ちなみに、fit_transform は学習と変換を同時に行うメソッドです。<br>
        PCA をするときなどに便利です。
      </p>
    </section>
    <section>
      <h2>3. 相関係数が閾値以下の変数を削除するクラスを作る</h2>
      <p>
        fit すると各変数と観測値との相関係数の絶対値を算出して覚えておいてくれる。<br>
        transform すると相関係数が閾値以下のものを削除したデータを返してくれる。<br>
        そんなクラスを作るべく、上記PCAのスクリプトをもとに、下記のように書いてみました。<br>
        <pre><code class="prettyprint lang-py">
from sklearn.base import BaseEstimator
import numpy as np
import pandas as pd
class rmvSmallCorr(BaseEstimator):
    def __init__(self, thr=.01):
        self.thr = thr

    def fit(self, X, y):
        self._fit(X, y)
        return self

    def _fit(self, X, y):
        from scipy.stats import pearsonr
        if isinstance(X, pd.core.frame.DataFrame): X = X.values
        corrs = []
        for i in range(0, X.shape[1]):
            corrs.append(abs(pearsonr(X[:, i], y)[0]))
        self.idx_rmv = np.array(corrs) > self.thr
        return X

    def transform(self, X):
        if isinstance(X, pd.core.frame.DataFrame): X = X.values
        X = X[:, ~self.idx_rmv]
        return X

    def fit_transform(self, X, y):
        X = self._fit(X, y)
        X = X[:, ~self.idx_rmv]
        return X
        </code></pre>
        Pipeline からの GridSearchCV に突っ込んでみたところ、うまいこと動いてくれました。<br>
        他にもいろいろ応用がききそうです。
      <aside class="ts">
        <h4>TROUBLE SHOOTING: AttributeError: rmvSmallCorr instance has no attribute 'get_params'</h4>
          class の引数にBaseEstimator を入れて、import BaseEstimator してやらないと、このエラーが返ってきます<br>
      </aside>　
    <section>
      <h2>4. グリッドサーチの進捗を表示する</h2>
      <p>
        <a href="https://github.com/tqdm/tqdm">
        tqdm</a> というライブラリを導入することで、繰り返し処理の進捗度合いを簡単に確認することができます。<br>
        グリッドサーチにはかなり時間がかかる場合もあるので、ぜひとも進捗を表示したいのですが、その機能はないようです。<br>
        そこでちょっと無理やりに、進捗を表示させる方法を考えてみました。<br>
      </p><p>
        上記の要領で、logProg というダミーのクラスを作り、Pipeline の頭に組み入れます。<br>
        logProg で fit や transform をしても、データには何も手が加わりません。<br>
        しかし、デスクトップにテキストファイルを作り、「1」と書き込むという動作をします。<br>
        すでにテキストファイルがある場合は、それに「1」を追記します。<br>
        これによって、テキストファイルにいくつ「1」と書き込まれているのかを見れば、現在グリッドサーチが何回転したかを確認できます。<br>
        ちなみに、このテキストファイルへの書き込みによって進捗を記録する方法は、昔MATLABで parfor の進捗を表示できないかと調べたときに行き着いたものです。<br>
        どこのページなのか失念してしまい参考文献リストにあげられませんが、ご容赦ください。
      </p><p>
        この設計を実現すべく、下記のようなスクリプトを書いてみました。<br>
        <pre><code class="prettyprint lang-py">
from sklearn.base import BaseEstimator
class est_logProg(BaseEstimator):
    from sklearn.base import BaseEstimator
    def __init__(self, path='C:\\Users\\issakuss\\Desktop\\prog.txt'):
        self.path = path

    def fit(self, X, y):
        return self

    def _fit(self, X, y):
        return X

    def transform(self, X):
        return X

    def fit_transform(self, X, y):
        f = open(self.path, 'a')
        f.write('1')
        f.close()
        return X
        </pre></code>
      </p><p>
        これだけだとただテキストファイルに書き込むだけです。<br>
        テキストファイルを参照して、進捗度合いを表示するスクリプトを別で書きます<br>
        （Jupyter notebook を使っています）。<br>
        <pre><code class="prettyprint lang-py">
from tqdm import tqdm_notebook as tqdm
from copy import deepcopy
from time import sleep
from os import remove
path = 'C:\\Users\\issakuss\\Desktop\\prog.txt'
total = 20
p = tqdm(total=total)
cur = 0
f = open(path, 'r')
while cur < total:
    old = deepcopy(cur)
    cur = f.read()
    cur = len(cur)
    p.update(cur - old)
    sleep(1.)
f.close()
os.remove(path)
        </pre></code>
        total の変数には、繰り返し数を手入力してやります。<br>
        sleep の時間をある程度とってやらないとうまく動きませんでした。<br>
        GridSearchCV に n_jobs を指定して並列処理なんかをさせると、きっちりと進捗表示できない場合もあります。<br>
        しかし大体の状況を知るには十分機能してくれます。
      </p><p>
        色々と泥臭いスクリプトなので改善の余地もありそうです。<br>
        またひょっとしたら、Scikit-learn の基本機能をちゃんと生かしてやる手もあるかもしれません。
    </section>
    <footer>
      <hr>
      <p>
      トップページへ戻る：
      <a href="../index.html">
        川島一朔のHP</a>
      </p>
    </footer>
    <script>
      prettyPrint();
    </script>
  </body>
</html>
